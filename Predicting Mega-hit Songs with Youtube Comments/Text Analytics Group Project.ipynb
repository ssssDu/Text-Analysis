{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/IPython/core/magics/pylab.py:160: UserWarning: pylab import has clobbered these variables: ['shape', 'summer']\n",
      "`%matplotlib` prevents importing * from pylab and numpy\n",
      "  \"\\n`%matplotlib` prevents importing * from pylab and numpy\"\n"
     ]
    }
   ],
   "source": [
    "#Import packages\n",
    "import pandas as pd\n",
    "%pylab inline\n",
    "import datetime\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "import nltk\n",
    "from string import punctuation\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in dataset scripted from YouTube.com"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read in reviews - Non-hits\n",
    "castle = pd.read_csv('/Users/Documents/YoutubeScrapes/Ed Sheeran - Castle on Hill.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "roses = pd.read_csv('/Users/Documents/YoutubeScrapes/Chainsmokers - Roses.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "congrats = pd.read_csv('/Users/Documents/YoutubeScrapes/Post Malone Congratulations Comments.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "dangerous = pd.read_csv('/Users/Documents/YoutubeScrapes/Ariana Grande - Dangerous Woman.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "kiss = pd.read_csv('/Users/Documents/YoutubeScrapes/Rihanna Kiss it Better comments.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "dna = pd.read_csv('/Users/Documents/YoutubeScrapes/Kendrick Lamar - DNA.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "motor = pd.read_csv('/Users/Documents/YoutubeScrapes/Migos - Motorsport.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "#ifeel = pd.read_csv('/Users/Documents/YoutubeScrapes/i feel it coming.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "summer = pd.read_csv('/Users/Documents/YoutubeScrapes/Drake - Summer Sixteen.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#Read in review - Hits\n",
    "shape = pd.read_csv('/Users/Documents/YoutubeScrapes/EdSheeran - Shape of You.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "closer = pd.read_csv('/Users/Documents/YoutubeScrapes/Chainsmokers - Closer.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "rockstar = pd.read_csv('/Users/Documents/YoutubeScrapes/Post Malone Rockstar Comments.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "side = pd.read_csv('/Users/Documents/YoutubeScrapes/Ariana Grande - Side to Side.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "work = pd.read_csv('/Users/Documents/YoutubeScrapes/Rihanna Work.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "humble = pd.read_csv('/Users/Documents/YoutubeScrapes/Kendrick Lamar - HUMBLE..csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "bad = pd.read_csv('/Users/Documents/YoutubeScrapes/Migos - Bad and Boujee.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "#star = pd.read_csv('/Users/Documents/YoutubeScrapes/The weeknd - Starboy.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "dance = pd.read_csv('/Users/Documents/YoutubeScrapes/Drake - One Dance.csv', usecols = ['id', 'user', 'date', 'timestamp', 'commentText', 'likes', 'hasReplies', 'numberOfReplies'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get rid of NA values from rows with replies\n",
    "castle = castle.dropna()\n",
    "roses = roses.dropna()\n",
    "congrats = congrats.dropna()\n",
    "shape = shape.dropna()\n",
    "closer = closer.dropna()\n",
    "rockstar = rockstar.dropna()\n",
    "dangerous = dangerous.dropna()\n",
    "side = side.dropna()\n",
    "kiss = kiss.dropna()\n",
    "work = work.dropna()\n",
    "humble = humble.dropna()\n",
    "dna = dna.dropna()\n",
    "motor = motor.dropna()\n",
    "bad = bad.dropna()\n",
    "dance = dance.dropna()\n",
    "summer = summer.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     id              user        date     timestamp  \\\n",
      "0  UghiYg2QkhaUfngCoAEC             YAZAN  1 year ago  1.507489e+12   \n",
      "1  UggJW-576f74q3gCoAEC            Iliana  1 year ago  1.507489e+12   \n",
      "2  UggExP4NiQsupHgCoAEC  Dean Desrouleaux  1 year ago  1.507489e+12   \n",
      "3  Ugi5Ehdcds9ZmHgCoAEC     Sylvia Pedone  1 year ago  1.507489e+12   \n",
      "4  UggWD_ALi_iycXgCoAEC      Bradley Venn  1 year ago  1.507489e+12   \n",
      "5  UghFoYsSJKtzN3gCoAEC  Vaibhav Bhardwaj  1 year ago  1.507489e+12   \n",
      "6  Ugj1Uphn-KrrKngCoAEC       anna banana  1 year ago  1.507489e+12   \n",
      "7  UggudnnZK-3ZPHgCoAEC               ÊûóÂ•ïÊ±ù  1 year ago  1.507489e+12   \n",
      "8  Ugie-2d3XzEWpXgCoAEC    Kate Sognatore  1 year ago  1.507489e+12   \n",
      "9  UgiLufEL7U5Jb3gCoAEC     Sammy Escobar  1 year ago  1.507489e+12   \n",
      "\n",
      "                                         commentText  likes hasReplies  \\\n",
      "0                                                yes    0.0      False   \n",
      "1                                            oh my..    0.0      False   \n",
      "2                                 The King Is Backüôåüèæ    0.0      False   \n",
      "3                                         Amazing‚ù§üëèüèª    0.0      False   \n",
      "4                                              First    0.0      False   \n",
      "5              that drinking scene at 4:26 was good!    1.0      False   \n",
      "6  I am so fucking proud if you. I've been a fan ...    0.0      False   \n",
      "7                              when we were young...    0.0      False   \n",
      "8                                         üòç‚ù§ finally    0.0      False   \n",
      "9                            my emotions shhakaoqpsb    0.0      False   \n",
      "\n",
      "   numberOfReplies                      song  Hit_or_No  \n",
      "0              0.0  Castle on the Hill (EdS)          0  \n",
      "1              0.0  Castle on the Hill (EdS)          0  \n",
      "2              0.0  Castle on the Hill (EdS)          0  \n",
      "3              0.0  Castle on the Hill (EdS)          0  \n",
      "4              0.0  Castle on the Hill (EdS)          0  \n",
      "5              0.0  Castle on the Hill (EdS)          0  \n",
      "6              0.0  Castle on the Hill (EdS)          0  \n",
      "7              0.0  Castle on the Hill (EdS)          0  \n",
      "8              0.0  Castle on the Hill (EdS)          0  \n",
      "9              0.0  Castle on the Hill (EdS)          0  \n"
     ]
    }
   ],
   "source": [
    "#define number of comments we want\n",
    "n = 5000\n",
    "\n",
    "#Select comments for all reviews\n",
    "castle = castle[-n:]\n",
    "roses = roses[-n:]\n",
    "congrats = congrats[-n:]\n",
    "shape = shape[-n:]\n",
    "closer = closer[-n:]\n",
    "rockstar = rockstar[-n:]\n",
    "dangerous = dangerous[-n:]\n",
    "side = side[-n:]\n",
    "kiss = kiss[-n:]\n",
    "work = work[-n:]\n",
    "dna = dna[-n:]\n",
    "humble = humble[-n:]\n",
    "bad = bad[-n:]\n",
    "motor = motor[-n:]\n",
    "summer = summer[-n:]\n",
    "dance = dance[-n:]\n",
    "\n",
    "#create new column indicating song\n",
    "castle['song'] = 'Castle on the Hill (EdS)'\n",
    "castle['Hit_or_No'] = 0\n",
    "roses['song'] = 'Roses (ChainS)'\n",
    "roses['Hit_or_No'] = 0\n",
    "congrats['song'] = 'Congratulations (PostM)'\n",
    "congrats['Hit_or_No'] = 0\n",
    "shape['song'] = 'Shape of You (EdS)'\n",
    "shape['Hit_or_No'] = 1\n",
    "closer['song'] = 'Closer (ChainS)'\n",
    "closer['Hit_or_No'] = 1\n",
    "rockstar['song'] = 'Rockstar (PostM)'\n",
    "rockstar['Hit_or_No'] = 1\n",
    "side['song'] = 'Side To Side (ArianaG)'\n",
    "side['Hit_or_No'] = 1\n",
    "dangerous['song'] = 'Dangerous Woman (ArianaG)'\n",
    "dangerous['Hit_or_No'] = 0\n",
    "kiss['song'] = 'Kiss It Better (Rihanna)'\n",
    "kiss['Hit_or_No'] = 0\n",
    "work['song'] = 'Work (Rihanna)'\n",
    "work['Hit_or_No'] = 1\n",
    "dna['song'] = 'DNA (KendrickL)'\n",
    "dna['Hit_or_No'] = 0\n",
    "humble['song'] = 'Humble (KendrickL)'\n",
    "humble['Hit_or_No'] = 1\n",
    "motor['song'] = 'Motorsport (Migos)'\n",
    "motor['Hit_or_No'] = 0\n",
    "bad['song'] = 'Bad and Boujee (Migos)'\n",
    "bad['Hit_or_No'] = 1\n",
    "summer['song'] = 'Summer Sixteen (Drake)'\n",
    "summer['Hit_or_No'] = 0\n",
    "dance['song'] = 'One Dance (Drake)'\n",
    "dance['Hit_or_No'] = 1\n",
    "\n",
    "#merge all comments into their own dataframe\n",
    "merged = pd.concat([castle, roses, congrats, shape, closer, rockstar, side, dangerous, kiss, work, dna, humble, motor, bad, summer, dance], ignore_index=True)\n",
    "\n",
    "\n",
    "print(merged[0:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remove punctuation & change everything to lowercase\n",
    "def lower_text(s):\n",
    "    for p in punctuation:\n",
    "        s = s.replace(p,'')\n",
    "    return s.lower()\n",
    "\n",
    "merged['commentText'] = merged['commentText'].map(lower_text)\n",
    "\n",
    "#Remove stopwords\n",
    "from nltk.corpus import stopwords\n",
    "stopwords = stopwords.words('english')\n",
    "def stop(string):\n",
    "    string = \" \"+string+\" \"\n",
    "    for word in stopwords:\n",
    "        string = string.replace(\" \"+str(word)+\" \", ' ') \n",
    "    return string\n",
    "\n",
    "merged['no_stop'] = merged['commentText'].map(stop)\n",
    "\n",
    "#Tokenizing & Lemmatizing \n",
    "w_tokenizer = nltk.tokenize.WhitespaceTokenizer()\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "def lemmatize_stem_text(text):\n",
    "    return [lemmatizer.lemmatize(w) for w in w_tokenizer.tokenize(text)]\n",
    "\n",
    "merged['Lemmatize'] = merged['no_stop'].apply(lemmatize_stem_text).str.join(' ')\n",
    "\n",
    "#Part of Speech Tag\n",
    "def tokenize(s):\n",
    "    return nltk.pos_tag(word_tokenize(s))\n",
    "\n",
    "merged['Tokens'] = merged['Lemmatize'].map(tokenize)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply function to youTube Comments\n",
    "merged_hit['commentText'] = merged_hit['commentText'].map(lower_text)\n",
    "merged_non_hit['commentText'] = merged_non_hit['commentText'].map(lower_text)\n",
    "migos['commentText'] = migos['commentText'].map(lower_text)\n",
    "post['commentText'] = post['commentText'].map(lower_text)\n",
    "Rihanna['commentText'] = Rihanna['commentText'].map(lower_text)\n",
    "chain['commentText'] = chain['commentText'].map(lower_text)\n",
    "\n",
    "merged_hit['no_stop'] = merged_hit['commentText'].map(stop)\n",
    "merged_non_hit['no_stop'] = merged_non_hit['commentText'].map(stop)\n",
    "migos['no_stop'] = migos['commentText'].map(stop)\n",
    "post['no_stop'] = post['commentText'].map(stop)\n",
    "Rihanna['no_stop'] = Rihanna['commentText'].map(stop)\n",
    "chain['no_stop'] = chain['commentText'].map(stop)\n",
    "\n",
    "merged_hit['Lemmatize'] =merged_hit['no_stop'].apply(lemmatize_stem_text).str.join(' ')\n",
    "merged_non_hit['Lemmatize'] =merged_non_hit['no_stop'].apply(lemmatize_stem_text).str.join(' ')\n",
    "migos['Lemmatize'] =migos['no_stop'].apply(lemmatize_stem_text).str.join(' ')\n",
    "post['Lemmatize'] =post['no_stop'].apply(lemmatize_stem_text).str.join(' ')\n",
    "Rihanna['Lemmatize'] =Rihanna['no_stop'].apply(lemmatize_stem_text).str.join(' ')\n",
    "chain['Lemmatize'] =chain['no_stop'].apply(lemmatize_stem_text).str.join(' ')\n",
    "\n",
    "merged_hit['Tokens'] = merged_hit['Lemmatize'].map(tokenize)\n",
    "merged_non_hit['Tokens'] = merged_non_hit['Lemmatize'].map(tokenize)\n",
    "migos['Tokens'] = migos['Lemmatize'].map(tokenize)\n",
    "post['Tokens'] = post['Lemmatize'].map(tokenize)\n",
    "Rihanna['Tokens'] = Rihanna['Lemmatize'].map(tokenize)\n",
    "chain['Tokens'] = chain['Lemmatize'].map(tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bag of Words on the hit data\n",
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "import numpy as np\n",
    "np.random.seed(2018)\n",
    "dictionary_hit = gensim.corpora.Dictionary(merged_hit['Tokens'])\n",
    "count = 0\n",
    "for k, v in dictionary_hit.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        \n",
    "#Bag of Words on the non_hit data\n",
    "np.random.seed(2018)\n",
    "dictionary_non_hit = gensim.corpora.Dictionary(merged_non_hit['Tokens'])\n",
    "count = 0\n",
    "for k, v in dictionary_non_hit.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        \n",
    "#Bag of Words on the migos\n",
    "np.random.seed(2018)\n",
    "dictionary_migos = gensim.corpora.Dictionary(migos['Tokens'])\n",
    "count = 0\n",
    "for k, v in dictionary_migos.iteritems():\n",
    "    print(k, v)\n",
    "    count += 1\n",
    "    if count > 10:\n",
    "        break\n",
    "        \n",
    "#Filter out tokens that appear\n",
    "dictionary_hit.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "dictionary_non_hit.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "dictionary_migos.filter_extremes(no_below=15, no_above=0.5, keep_n=100000)\n",
    "\n",
    "#Gensim doc2bow\n",
    "bow_corpus_hit = [dictionary_hit.doc2bow(doc) for doc in merged_hit['Tokens']]\n",
    "bow_corpus_non_hit= [dictionary_non_hit.doc2bow(doc) for doc in merged_non_hit['Tokens']]\n",
    "bow_corpus_migos= [dictionary_migos.doc2bow(doc) for doc in migos['Tokens']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Run LDA using Bag of Words for hit\n",
    "lda_model = gensim.models.LdaMulticore(bow_corpus_hit, num_topics=5, id2word=dictionary_hit, passes=50)\n",
    "#for idx, topic in lda_model.print_topics(-1):\n",
    "#    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Running LDA using Bag of Words for non_hit\n",
    "lda_model_non = gensim.models.LdaMulticore(bow_corpus_non_hit, num_topics=5, id2word=dictionary_non_hit, passes=50)\n",
    "#for idx, topic in lda_model_non.print_topics(-1):\n",
    "#    print('Topic: {} \\nWords: {}'.format(idx, topic))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Frequent Words in Hit & Non-hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get lists of all the tokenized words in all posts\n",
    "Hit = merged[merged['Hit_or_No']==1]\n",
    "Non_hit = merged[merged['Hit_or_No']==0]\n",
    "\n",
    "hit_list = list()\n",
    "non_hit_list = list()\n",
    "\n",
    "for items in Non_hit['Tokens']:\n",
    "    non_hit_list = non_hit_list + items\n",
    "\n",
    "for items in Hit['Tokens']:\n",
    "    hit_list = hit_list + items\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find NN  -- Nouns \n",
    "def takeNN(X):\n",
    "    NNwords =[]\n",
    "    for word,tag in X:\n",
    "        if tag == 'NN':\n",
    "            NNwords.append(word)\n",
    "    return NNwords\n",
    "\n",
    "# Get a list of NN words\n",
    "hit_NN = pd.Series(takeNN(hit_list))\n",
    "non_hit_NN = pd.Series(takeNN(non_hit_list))\n",
    "\n",
    "unhelpfulwords = ['song','view','video','im','post','ed','rockstar','malone','halsey', 'music', 'i', 'congratulation', 'quavo', 'drake', 'migos','motorsport', 'ariana', 'grande','side','kendrick','lamar','summer', 'rihanna', 'lil','riri','uzi' ]\n",
    "hit_NN = hit_NN[~hit_NN.isin(unhelpfulwords)]\n",
    "non_hit_NN = non_hit_NN[~non_hit_NN.isin(unhelpfulwords)]\n",
    "\n",
    "hit_attribute_count = hit_NN.value_counts()\n",
    "#plt.hist(hit_attribute_count[0:20])\n",
    "#plt.xlabel(hit_attribute_count[0:20].index.values)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_hit_attribute_count = non_hit_NN.value_counts()\n",
    "#plt.hist(non_hit_attribute_count[0:20])\n",
    "#plt.xlabel(non_hit_attribute_count[0:20].index.values)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictive Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "from sklearn import svm\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from patsy import dmatrices\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "#Get counts\n",
    "count_vect = CountVectorizer()\n",
    "X_counts = count_vect.fit_transform(merged.Lemmatize)\n",
    "\n",
    "#transform to tf-idf\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_tfidf = tfidf_transformer.fit_transform(X_counts)\n",
    "\n",
    "#get Ys\n",
    "y = merged['hit'].values\n",
    "\n",
    "#split into train and test (pull out Ariana Grande hit and flop to be testing data)\n",
    "X_train = scipy.sparse.vstack([X_tfidf[25:50], X_tfidf[75:900], X_tfidf[925:975]])\n",
    "X_test = scipy.sparse.vstack([X_tfidf[0:25],X_tfidf[50:75],X_tfidf[900:925],X_tfidf[975:]])\n",
    "y_train = np.append(y[25:50], y[75:900])\n",
    "y_train = np.append(y_train, y[925:975])\n",
    "y_test = np.append(y[0:25], y[50:75]) \n",
    "y_test = np.append(y_test, y[900:925])\n",
    "y_test = np.append(y_test,y[975:])\n",
    "X_train_C = scipy.sparse.vstack([X_counts[25:50], X_counts[75:900], X_counts[925:975]])\n",
    "X_test_C = scipy.sparse.vstack([X_counts[0:25],X_counts[50:75],X_counts[900:925],X_counts[975:]])\n",
    "\n",
    "# Fit models\n",
    "clf = BernoulliNB().fit(X_train, y_train)\n",
    "clf2 = MultinomialNB(alpha = 1).fit(X_train_C, y_train)\n",
    "\n",
    "#Make predictions on test\n",
    "predicted = clf.predict(X_test)\n",
    "predicted2 = clf2.predict(X_test_C)\n",
    "\n",
    "#print (\"The accuracy of the text-only model is:\")\n",
    "#print (metrics.accuracy_score(y_test, predicted))\n",
    "\n",
    "#print (\"The accuracy of the Multinomial NB  model is:\")\n",
    "#print (metrics.accuracy_score(y_test, predicted2))\n",
    "\n",
    "#print ('Prior probability for the negative class is:')\n",
    "#print (exp(clf.class_log_prior_[0]))\n",
    "\n",
    "#print (\"The confusion matrix for this model can be seen below:\")\n",
    "#print(confusion_matrix(y_test, predicted))\n",
    "\n",
    "#print (\"The confusion matrix for Multinomial NB can be seen below:\")\n",
    "#print(confusion_matrix(y_test, predicted2))\n",
    "\n",
    "#Get the most important features in the model\n",
    "names = count_vect.get_feature_names()\n",
    "inter_class_differences = clf.feature_log_prob_[1] - clf.feature_log_prob_[0]\n",
    "new_feature_importance_series = pd.Series(inter_class_differences, names)\n",
    "\n",
    "#print(\"\\nFeatures most likely to indicate a hit:\")\n",
    "#print(new_feature_importance_series.sort_values(ascending=False)[:15])\n",
    "\n",
    "#print(\"\\nFeatures most likely to indicate a non-hit:\")\n",
    "#print(new_feature_importance_series.sort_values(ascending=False)[-15:])\n",
    "\n",
    "#Get the most important features in the Multinomial model\n",
    "inter_class_differences_MN = clf2.feature_log_prob_[1] - clf2.feature_log_prob_[0]\n",
    "new_feature_importance_series_MN = pd.Series(inter_class_differences_MN, names)\n",
    "\n",
    "#print(\"\\nMultinomial Features most likely to indicate a hit:\")\n",
    "#print(new_feature_importance_series_MN.sort_values(ascending=False)[:15])\n",
    "\n",
    "#print(\"\\nMultinomial Features most likely to indicate a non-hit:\")\n",
    "#print(new_feature_importance_series_MN.sort_values(ascending=False)[-15:])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiple files into one big dataframe\n",
    "import nltk\n",
    "import os\n",
    "nltk.download('movie_reviews')",
    "nltk.data.find('corpora/movie_reviews')\n",
    "\n",
    "#mega hits - one big dataframe\n",
    "path_mega = r'Mega_hits'\n",
    "files_mega = glob.iglob(os.path.join(path_mega, \"*.csv\"))    \n",
    "\n",
    "df_each_mega = (pd.read_csv(f, header = 0, encoding = 'utf8').dropna(subset=['commentText']).iloc[-5000:,1:8].drop(columns=['date','timestamp','hasReplies']).assign(Song=os.path.basename(f)) for f in files_mega)\n",
    "df_mega = pd.concat(df_each_mega, ignore_index=True)\n",
    "df_mega['hit'] = 1\n",
    "df_mega['Song'] = df_mega['Song'].str.replace('.csv','')\n",
    "df_mega['Artist']=df_mega.Song.apply(lambda x: x.split('-')[0])\n",
    "df_mega['Song_split']=df_mega.Song.apply(lambda x: x.split('-')[1])\n",
    "\n",
    "\n",
    "#non mega hits - one big dataframe\n",
    "path_non = r'Non_hits'\n",
    "files_non = glob.iglob(os.path.join(path_non, \"*.csv\"))     \n",
    "\n",
    "df_each_non = (pd.read_csv(f, header = 0, encoding = 'utf8').dropna(subset=['commentText']).iloc[-5000:,1:8].drop(columns=['date','timestamp','hasReplies']).assign(Song=os.path.basename(f)) for f in files_non)\n",
    "df_non = pd.concat(df_each_non, ignore_index=True)\n",
    "df_non['hit'] = 0\n",
    "df_non['Song'] = df_non['Song'].str.replace('.csv','')\n",
    "df_non['Artist']=df_non.Song.apply(lambda x: x.split('-')[0])\n",
    "df_non['Song_split']=df_non.Song.apply(lambda x: x.split('-')[1])\n",
    "\n",
    "# Processing\n",
    "def word_feats(words):\n",
    "    return dict([(word, True) for word in words])\n",
    "\n",
    "negids = movie_reviews.fileids('neg')\n",
    "posids = movie_reviews.fileids('pos')\n",
    "negfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'neg') for f in negids]\n",
    "posfeats = [(word_feats(movie_reviews.words(fileids=[f])), 'pos') for f in posids]\n",
    "trainfeats = negfeats + posfeats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "classifier = NaiveBayesClassifier.train(trainfeats)\n",
    "merged['tokenized'] = merged['commentText'].apply(lambda x: [t.lower().encode('utf-8').strip(\":,.!?\") for t in x.split()] )\n",
    "merged['sentiment_mega'] = merged['tokenized'].apply(lambda x: classifier.prob_classify(word_feats(x)).prob('pos') - classifier.prob_classify(word_feats(x)).prob('neg') )\n",
    "\n",
    "df_non['tokenized'] = df_non['commentText'].apply(lambda x: [t.lower().encode('utf-8').strip(\":,.!?\") for t in x.split()] )\n",
    "df_non['sentiment_non'] = df_non['tokenized'].apply(lambda x: classifier.prob_classify(word_feats(x)).prob('pos') - classifier.prob_classify(word_feats(x)).prob('neg') )\n",
    "\n",
    "grouped_mega = df_mega.groupby(['Artist'])\n",
    "y = grouped_mega.sentiment_mega.aggregate(np.sum)\n",
    "\n",
    "grouped_non = df_non.groupby(['Artist'])\n",
    "x = grouped_non.sentiment_non.aggregate(np.sum)\n",
    "\n",
    "combined = pd.concat([y,x],axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overview\n",
    "neg_sent_mask = df_mega['sentiment_mega'] < -0.3\n",
    "#df_mega[neg_sent_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Clouds for Each Songs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#WordCloud Function\n",
    "def fancySentiment(comments):\n",
    "    stopword = set(stopwords.words('english') + list(string.punctuation) + ['n\\'t'])\n",
    "    filtered_comments = []\n",
    "    for i in comments:\n",
    "        i.lower()\n",
    "        words = word_tokenize(i)\n",
    "        temp_filter = \"\"\n",
    "        for w in words:\n",
    "            if w not in stopword and w not in customstopwords:\n",
    "                temp_filter += str(w)\n",
    "                temp_filter += ' '\n",
    "        filtered_comments.append(temp_filter)\n",
    "    filtered_comments_str = ' '.join(filtered_comments) \n",
    "    sentiment = WordCloud(background_color = 'orange', max_words=100)\n",
    "    sentiment.generate(filtered_comments_str)\n",
    "    plt.figure(figsize=(12, 15))\n",
    "    plt.imshow(sentiment)\n",
    "    plt.axis(\"off\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Mega Hits Word Cloud\n",
    "for filepath in glob.iglob('*.csv'):\n",
    "    df = pd.read_csv(filepath, header = 0, encoding = 'utf8')\n",
    "    df = df.dropna(subset=['commentText'])\n",
    "    df['commentText'] = df['commentText'].str.lower()\n",
    "    df = df.iloc[-5000:,1:8].copy()\n",
    "    df = df.drop(columns=['date','timestamp','hasReplies'])\n",
    "    song = filepath.replace('Mega_hits','')\n",
    "    song = song.replace('.csv','')\n",
    "    df['song'] = song\n",
    "    df['hit'] = 1    \n",
    "    d[\"df\" + str(count)] = df.copy()\n",
    "    d[\"df\" + str(count)]['commentText'] = d[\"df\" + str(count)]['commentText'].apply(lambda x: safe_str(x))\n",
    "    fanc = d[\"df\" + str(count)]['commentText']\n",
    "    #fancySentiment(fanc)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Non Mega hits Word Cloud\n",
    "for filepath in glob.iglob('*.csv'):\n",
    "    \n",
    "    #assign df to each song & clean the file by removing blanks and selecting fewer columns\n",
    "    df = pd.read_csv(filepath, header = 0, encoding = 'utf8')\n",
    "    df = df.dropna(subset=['commentText'])\n",
    "    df['commentText'] = df['commentText'].str.lower()\n",
    "    \n",
    "    #select last 5,000 comments\n",
    "    df = df.iloc[-5000:,1:8].copy()\n",
    "    df = df.drop(columns=['date','timestamp','hasReplies'])\n",
    "    \n",
    "    #pull the file name to create a song column\n",
    "    song = filepath.replace('Non_hits','')\n",
    "    song = song.replace('.csv','')\n",
    "    df['song'] = song\n",
    "    \n",
    "    #mark this section as non-hits = 0\n",
    "    df['hit'] = 0\n",
    "    \n",
    "    #create separate dataframe for each loop. df1, df2 df3...\n",
    "    d[\"df\" + str(count)] = df.copy()\n",
    " \n",
    "    #cleans text column & create word cloud\n",
    "    d[\"df\" + str(count)]['commentText'] = d[\"df\" + str(count)]['commentText'].apply(lambda x: safe_str(x))\n",
    "    fanc = d[\"df\" + str(count)]['commentText']\n",
    "    #fancySentiment(fanc)\n",
    "    count += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
